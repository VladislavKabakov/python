{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9398cbd",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c6a6008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\v\\Desktop\\python\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from time import time\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import rcParams\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3c6e3e",
   "metadata": {},
   "source": [
    "### 1. Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e3b06f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "lesions = []\n",
    "root = 'PH2Dataset'\n",
    "\n",
    "for root, dirs, files in os.walk(os.path.join(root, 'PH2 Dataset images')):\n",
    "    if root.endswith('_Dermoscopic_Image'):\n",
    "        images.append(imread(os.path.join(root, files[0])))\n",
    "    if root.endswith('_lesion'):\n",
    "        lesions.append(imread(os.path.join(root, files[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff4cd9b",
   "metadata": {},
   "source": [
    "### 2. Resizing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a12c764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (256, 256)\n",
    "X = [resize(x, size, mode='constant', anti_aliasing=True,) for x in images]\n",
    "Y = [resize(y, size, mode='constant', anti_aliasing=False) > 0.5 for y in lesions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9c7f846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 images\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X, np.float32)\n",
    "Y = np.array(Y, np.float32)\n",
    "print(f'Loaded {len(X)} images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a08d6d",
   "metadata": {},
   "source": [
    "#### Showing some data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1bc1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 6))\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 6, i+1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(X[i])\n",
    "\n",
    "    plt.subplot(2, 6, i+7)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(Y[i])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e62898",
   "metadata": {},
   "source": [
    "### 3. Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026b29fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = np.random.choice(len(X), len(X), False)\n",
    "tr, val, ts = np.split(ix, [100, 150])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a8aa5e",
   "metadata": {},
   "source": [
    "#### Creating DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a235d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "data_tr = DataLoader(list(zip(np.rollaxis(X[tr], 3, 1), Y[tr, np.newaxis])), \n",
    "                     batch_size=batch_size, shuffle=True)\n",
    "data_val = DataLoader(list(zip(np.rollaxis(X[val], 3, 1), Y[val, np.newaxis])),\n",
    "                      batch_size=batch_size, shuffle=True)\n",
    "data_ts = DataLoader(list(zip(np.rollaxis(X[ts], 3, 1), Y[ts, np.newaxis])),\n",
    "                     batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dc2a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f49d5b2",
   "metadata": {},
   "source": [
    "## Creating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b51f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (15,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e8784f",
   "metadata": {},
   "source": [
    "### 1. SegNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fde72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SegNet, self).__init__()\n",
    "        \n",
    "        self.enc_conv0 = nn.Sequential(*[\n",
    "                                         nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(64),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(64),\n",
    "                                         nn.ReLU()\n",
    "                                        ])\n",
    "        self.pool0 =  nn.MaxPool2d(kernel_size=(2, 2), stride=2, return_indices=True)\n",
    "        \n",
    "        self.enc_conv1 = nn.Sequential(*[\n",
    "                                         nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(128),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(128),\n",
    "                                         nn.ReLU()\n",
    "                                        ])\n",
    "        self.pool1 =  nn.MaxPool2d(kernel_size=(2, 2), stride=2, return_indices=True)\n",
    "        \n",
    "        self.enc_conv2 = nn.Sequential(*[\n",
    "                                         nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(256),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(256),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(256),\n",
    "                                         nn.ReLU()\n",
    "                                        ])\n",
    "        self.pool2 =  nn.MaxPool2d(kernel_size=(2, 2), stride=2, return_indices=True)\n",
    "        \n",
    "        self.enc_conv3 = nn.Sequential(*[\n",
    "                                         nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(512),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(512),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(512),\n",
    "                                         nn.ReLU()\n",
    "                                         ])\n",
    "        self.pool3 =  nn.MaxPool2d(kernel_size=(2,2), stride=2, return_indices=True)\n",
    "        \n",
    "        self.bottleneck_conv_enc = nn.Sequential(*[\n",
    "                                         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(512),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(512),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(512),\n",
    "                                         nn.ReLU()\n",
    "                                         ])\n",
    "        self.bottleneck_pool = nn.MaxPool2d(kernel_size=(2,2), stride=2, return_indices=True)\n",
    "        self.bottleneck_upsample = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "        self.bottleneck_conv_dec = nn.Sequential(*[\n",
    "                                         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(512),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(512),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(512),     \n",
    "                                         nn.ReLU()\n",
    "                                         ])\n",
    "        \n",
    "        self.upsample0 =  nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "        self.dec_conv0 = nn.Sequential(*[\n",
    "                                         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(512),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(512),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(256),\n",
    "                                         nn.ReLU()\n",
    "                                         ]) \n",
    "        \n",
    "        self.upsample1 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "        self.dec_conv1 = nn.Sequential(*[\n",
    "                                         nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(256),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(256),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(128),\n",
    "                                         nn.ReLU()\n",
    "                                         ])\n",
    "        \n",
    "        self.upsample2 = nn.MaxUnpool2d(kernel_size=2, stride=2)  # 64 -> 128\n",
    "        self.dec_conv2 = nn.Sequential(*[\n",
    "                                         nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(128),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(64),\n",
    "                                         nn.ReLU()\n",
    "                                         ]) \n",
    "        \n",
    "        self.upsample3 = nn.MaxUnpool2d(kernel_size=2, stride=2) # 128 -> 256\n",
    "        self.dec_conv3 = nn.Sequential(*[\n",
    "                                         nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(64),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(in_channels=64, out_channels=1, kernel_size=3, padding=1),\n",
    "                            \n",
    "                                         ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "    size_0 = x.size()\n",
    "    e0, id0 = self.pool0(self.enc_conv0(x))\n",
    "        \n",
    "    size_1 = e0.size()\n",
    "    e1, id1 = self.pool1(self.enc_conv1(e0))\n",
    "\n",
    "    size_2 = e1.size()\n",
    "    e2, id2 = self.pool2(self.enc_conv2(e1))\n",
    "\n",
    "    size_3 = e2.size()\n",
    "    e3, id3 = self.pool3(self.enc_conv3(e2))\n",
    "\n",
    "    size_4 = e3.size()\n",
    "    bottle_enc, bottle_id = self.bottleneck_pool(self.bottleneck_conv_enc(e3))\n",
    "\n",
    "    size_d = bottle_enc.size()\n",
    "\n",
    "    bottle_dec = self.bottleneck_conv_dec(self.bottleneck_upsample(bottle_enc, bottle_id, output_size=size_4))\n",
    "\n",
    "    d0 = self.dec_conv0(self.upsample0(bottle_dec, id3, output_size=size_3))\n",
    "    \n",
    "    d1 = self.dec_conv1(self.upsample1(d0, id2, output_size=size_2))\n",
    "\n",
    "    d2 = self.dec_conv2(self.upsample2(d1, id1, output_size=size_1))\n",
    "    d3 = self.dec_conv3(self.upsample3(d2, id0, output_size=size_0)) \n",
    "    return d3     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ec0241",
   "metadata": {},
   "source": [
    "#### Metrics (IoU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b00be04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor):\n",
    "    outputs = outputs.squeeze(1).byte()\n",
    "    labels = labels.squeeze(1).byte()\n",
    "    SMOOTH = 1e-8\n",
    "    intersection = (outputs & labels).float().sum((1, 2))  \n",
    "    union = (outputs | labels).float().sum((1, 2))         \n",
    "    \n",
    "    iou = (intersection + SMOOTH) / (union + SMOOTH)\n",
    "    \n",
    "    thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10\n",
    "    \n",
    "    return thresholded "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096644ac",
   "metadata": {},
   "source": [
    "#### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e3123b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, opt, loss_fn, epochs, data_tr, data_val):\n",
    "     since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_iou = 0.0\n",
    "    \n",
    "    epoch_train_loss = []\n",
    "    epoch_val_loss = []\n",
    "    epoch_train_iou = []\n",
    "    epoch_val_iou = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, epochs))\n",
    "        print('-' * 10)\n",
    "    \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_iou = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    Y_pred = model(inputs)\n",
    "                    loss = loss_fn(labels, Y_pred)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.detach().item() * inputs.size(0)\n",
    "                Y_pred = torch.sigmoid(Y_pred)\n",
    "                Y_pred = torch.where(Y_pred > 0.5, 1, 0)\n",
    "                running_iou += iou_pytorch(Y_pred, labels).mean().item()\n",
    "                \n",
    "            \n",
    "            epoch_loss = running_loss / len(dataloaders[phase])\n",
    "            epoch_iou = running_iou / len(dataloaders[phase])\n",
    "            \n",
    "            if phase == 'train':\n",
    "                epoch_train_loss.append(epoch_loss)\n",
    "                epoch_train_iou.append(epoch_iou)\n",
    "            else:\n",
    "                epoch_val_loss.append(epoch_loss)\n",
    "                epoch_val_iou.append(epoch_iou)\n",
    "            \n",
    "            \n",
    "            if phase == 'val':\n",
    "                Y_pred = Y_pred.detach().cpu()\n",
    "                clear_output(wait=True)\n",
    "                for k in range(5):\n",
    "                    plt.subplot(2, 5, k+1)\n",
    "                    plt.imshow(np.rollaxis(inputs[k].cpu().numpy(), 0, 3), cmap='gray')\n",
    "                    plt.title('Real', fontsize=12)\n",
    "                    \n",
    "                    plt.axis('off')\n",
    "\n",
    "                    plt.subplot(2, 5, k+6)\n",
    "                    plt.imshow(Y_pred[k, 0], cmap='gray')\n",
    "                    plt.title('Output', fontsize=12)\n",
    "                    plt.axis('off')\n",
    "                plt.suptitle('val Loss: {:.4f} val IOU: {:.4f}'.format(epoch_loss, epoch_iou), fontsize=15)\n",
    "                plt.show()\n",
    "                \n",
    "            inputs, labels = inputs.cpu(), labels.cpu()\n",
    "            Y_pred, loss = Y_pred.cpu(), loss.cpu()\n",
    "            del inputs, labels, Y_pred, loss,\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            \n",
    "            if phase == 'val' and epoch_iou > best_iou:\n",
    "                best_iou = epoch_iou\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val IOU: {:4f}'.format(best_iou))\n",
    "    \n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    statistics = epoch_train_loss, epoch_train_iou, epoch_val_loss, epoch_val_iou\n",
    "    return model, statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9624efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    model.eval()  # testing mode\n",
    "    Y_pred = [X_batch for X_batch, _ in data]\n",
    "    return np.array(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc47707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(model, metric, data):\n",
    "    model.eval()  # testing mode\n",
    "    scores = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, Y_label in data:\n",
    "            Y_pred = model(X_batch.to(device))\n",
    "            Y_pred = torch.sigmoid(Y_pred)\n",
    "            Y_pred = torch.where(Y_pred > 0.5, 1, 0)\n",
    "\n",
    "            scores += metric(Y_pred, Y_label.to(device)).mean().item()\n",
    "\n",
    "    return scores/ len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34613469",
   "metadata": {},
   "source": [
    "### Getting results with different loss-functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e7128c",
   "metadata": {},
   "source": [
    "#### BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede7a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_loss(y_real, y_pred):\n",
    "    y_pred, y_real = torch.unsqueeze(y_pred, 1), torch.unsqueeze(y_real, 1)\n",
    "    \n",
    "    epsilon = 10 ** -45\n",
    "    result = - torch.mean(\n",
    "        y_real * torch.log(torch.sigmoid(y_pred) + epsilon) + (1 - y_real) * torch.log(1 - torch.sigmoid(y_pred) + epsilon)\n",
    "    )\n",
    "\n",
    "    return result\n",
    "    \n",
    "model = SegNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d10e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SegNet().to(device)\n",
    "max_epochs = 50\n",
    "optim = torch.optim.Adam(model_bce.parameters(), lr=1e-4) \n",
    "train(model, optim, bce_loss, max_epochs, data_tr, data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbc00ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_model(model, iou_pytorch, data_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba80e2d3",
   "metadata": {},
   "source": [
    "#### Dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cfe945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(y_real, y_pred):\n",
    "    smooth=1e-7\n",
    "    inputs = torch.flatten(torch.sigmoid(inputs))\n",
    "    targets = torch.flatten(targets)\n",
    "    \n",
    "    intersection = torch.sum(inputs * targets)\n",
    "    cardinality = torch.sum(inputs + targets)\n",
    "    \n",
    "    dice_score = 2. * intersection / (cardinality + smooth)\n",
    "    return torch.mean(1. - dice_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18764cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dice = SegNet().to(device)\n",
    "max_epochs = 50\n",
    "optimaizer = optim.Adam(model_dice.parameters(), lr=1e-3)\n",
    "train(model_dice, optimaizer, dice_loss, max_epochs, data_tr, data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b413b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_model(model_dice, iou_pytorch, data_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d99446",
   "metadata": {},
   "source": [
    "#### Focal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefd807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(y_real, y_pred, eps = 1e-8, gamma = 2):\n",
    "    y_pred, y_real = torch.unsqueeze(y_pred, 1), torch.unsqueeze(y_real, 1) \n",
    "    \n",
    "    result = -torch.mean(\n",
    "        ((1 - torch.sigmoid(y_pred)) ** gamma) * y_real * torch.log(torch.sigmoid(y_pred) + eps) + (1 - y_real) * torch.log(\n",
    "        1 - torch.sigmoid(y_pred) + eps))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da581c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_focal = SegNet().to(device)\n",
    "\n",
    "max_epochs = 40\n",
    "optimaizer = optim.Adam(model_focal.parameters(), lr=1e-4)\n",
    "train(model_focal, optimaizer, focal_loss, max_epochs, data_tr, data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7d6d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_model(model_focal, iou_pytorch, data_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591c56c8",
   "metadata": {},
   "source": [
    "### 2. UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ba1672",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "         self.enc_conv0 = nn.Sequential(*[\n",
    "                                         nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(64),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(64),\n",
    "                                         nn.ReLU()\n",
    "                                         ])\n",
    "        self.pool0 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.enc_conv1 = nn.Sequential(*[\n",
    "                                         nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(128),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(128),\n",
    "                                         nn.ReLU()\n",
    "                                         ])\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.enc_conv2 = nn.Sequential(*[\n",
    "                                         nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(256),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(256),\n",
    "                                         nn.ReLU()\n",
    "                                         ])\n",
    "        self.pool2 =  nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.enc_conv3 = nn.Sequential(*[\n",
    "                                         nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(512),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(512),\n",
    "                                         nn.ReLU()\n",
    "                                         ])\n",
    "        self.pool3 =  nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bottleneck_conv = nn.Sequential(*[\n",
    "                                         nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(1024),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(1024),\n",
    "                                         nn.ReLU()\n",
    "                                         ])\n",
    "\n",
    "        self.upsample0 = nn.Upsample(32)\n",
    "        self.dec_conv0 = nn.Sequential(*[\n",
    "                                         nn.Conv2d(in_channels=1024 + 512, out_channels=512, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(512),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(512),\n",
    "                                         nn.ReLU()\n",
    "                                         ])\n",
    "        \n",
    "        self.upsample1 = nn.Upsample(64)\n",
    "        self.dec_conv1 = nn.Sequential(*[\n",
    "                                         nn.Conv2d(in_channels=512 + 256, out_channels=256, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(256),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(256),\n",
    "                                         nn.ReLU()\n",
    "                                         ])\n",
    "        self.upsample2 = nn.Upsample(128)\n",
    "        self.dec_conv2 = nn.Sequential(*[\n",
    "                                         nn.Conv2d(in_channels=256 + 128, out_channels=128, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(128),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(128),\n",
    "                                         nn.ReLU()\n",
    "                                         ])\n",
    "        self.upsample3 = nn.Upsample(256\n",
    "        self.dec_conv3 = nn.Sequential(*[\n",
    "                                         nn.Conv2d(in_channels=128 + 64, out_channels=64, kernel_size=3, padding=1),\n",
    "                                         nn.BatchNorm2d(64),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(in_channels=64, out_channels=1, kernel_size=3, padding=1)\n",
    "                                         ])\n",
    "                                     \n",
    "    def forward(self, x):\n",
    "        # encoder\n",
    "        e0 = self.enc_conv0(x)\n",
    "        e1 = self.enc_conv1(self.pool0(e0))\n",
    "        e2 = self.enc_conv2(self.pool1(e1))\n",
    "        e3 = self.enc_conv3(self.pool2(e2))\n",
    "\n",
    "        # bottleneck\n",
    "        b = self.bottleneck_conv(self.pool3(e3))\n",
    "\n",
    "        # decoder\n",
    "        d0 = self.upsample0(b)\n",
    "        d0 = torch.cat([d0, e3], dim=1)\n",
    "        d0 = self.dec_conv0(d0)\n",
    "        \n",
    "        d1 = self.upsample1(d0)\n",
    "        d1 = torch.cat([d1, e2], dim=1)\n",
    "        d1 = self.dec_conv1(d1)\n",
    "\n",
    "        d2 = self.upsample2(d1)\n",
    "        d2 = torch.cat([d2, e1], dim=1)\n",
    "        d2 = self.dec_conv2(d2)\n",
    "\n",
    "        d3 = self.upsample3(d2)\n",
    "        d3 = torch.cat([d3, e0], dim=1)\n",
    "        d3 = self.dec_conv3(d3)# no activation\n",
    "        \n",
    "        return d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9799e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = UNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09687ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(unet_model, optim.Adam(unet_model.parameters()), bce_loss, 20, data_tr, data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_model(unet_model, iou_pytorch, data_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
